from flask import Flask, request, jsonify  # Import Flask components
from huggingface_hub import InferenceClient
from flask_cors import CORS
import json
import os
from backend.format_handling.save_to_json import save_json_format
from dotenv import load_dotenv
from backend.network.udp_client import receive_response_from_server_udp
from backend.network.tcp_client import receive_response_from_server_tcp
from threading import Thread

# --- Initialization of global variables ---

# .env variables
load_dotenv()

# Tokens
hf_token = os.getenv("HF_TOKEN")

# Models (split comma-separated list)
models = os.getenv("MODELS", "").split(",")

# Evaluator
evaluator_model = os.getenv("EVALUATOR_MODEL")

# Server configuration
TCP_SERVER_IP = os.getenv("TCP_SERVER_IP", "127.0.0.1")
TCP_SERVER_PORT = int(os.getenv("TCP_SERVER_PORT", 25569))
UDP_SERVER_IP = os.getenv("UDP_SERVER_IP", "127.0.0.1")
UDP_SERVER_PORT = int(os.getenv("UDP_SERVER_PORT", 25560))

# Flask variables
FLASK_HOST = os.getenv("FLASK_HOST", "0.0.0.0")
FLASK_PORT = int(os.getenv("FLASK_PORT", 5000))

# File paths
RESPONSES_JSON_PATH = os.getenv("RESPONSES_JSON_PATH", "../data/responses.json")
SCORES_JSON_PATH = os.getenv("SCORES_JSON_PATH", "../data/scores.json")

# other initializations
evaluator_client = None

instructions_response_llm = ("You are a PhD-level Computer Networks professor. "
                             "Give accurate and concise responses, so that a Computer Science student would understand."
                             "Use short paragraphs and provide small code snippets if you consider necessary. These "
                             "code snippets"
                             "can be joined by short comments, right next to the code, not before or after.")

# initialization of clients used later for calling the llms
clients = []
print("Initializing models...")
for model in models:
    try:
        clients.append(InferenceClient(model=model, token=hf_token))
        print(f"Loaded: {model}")
    except Exception as e:
        print(f"Error loading {model}: {e}")

# initialization of evaluator client used later for calling the evaluating llm
try:
    evaluator_client = InferenceClient(model=evaluator_model, token=hf_token)
    print(f"Loaded evaluator: {evaluator_model}")
except Exception as e:
    print(f"Error loading evaluator {evaluator_model}: {e}")

# initializing the flask app
app = Flask(__name__)
CORS(app)


@app.route('/ask', methods=['POST'])
def handle_question():
    # Get the question from the incoming JSON data
    data = request.get_json()
    user_question = data.get('question')

    # error handling
    if not user_question:
        return jsonify({"error": "No question provided"}), 400

    prompt_response = (f"{instructions_response_llm}"
                       f"{user_question}")

    responses_dict = {}
    threads = []

    def call_llm(client_llm,prompt):
        model_name = client_llm.model if hasattr(client_llm, 'model') else "Unknown Model"
        try:
            response = receive_response_from_server_tcp(TCP_SERVER_IP, TCP_SERVER_PORT, ["", prompt, model_name])
            responses_dict[model_name] = response
        except Exception as ex:
            responses_dict[model_name] = f"Error: {ex}"

    # Process the question using all clients
    for client in clients:
        t = Thread(target=call_llm, args=(client,prompt_response,))
        t.start()
        threads.append(t)

    # Wait for all threads to finish
    for t in threads:
        t.join()

    # Save the results to responses.json for the evaluator to use later
    save_json_format(responses_dict,RESPONSES_JSON_PATH)

    # Return the results as JSON
    return jsonify(responses_dict)


@app.route('/evaluate', methods=['POST'])
def run_evaluation():
    # Get the question from the incoming JSON data
    data = request.get_json()
    user_question = data.get('question')

    if not user_question:
        return jsonify({"status": "failure", "message": "No question provided for evaluation"}), 400

    if not os.path.exists(RESPONSES_JSON_PATH):
        return jsonify({
            "status": "failure",
            "message": f"Responses file not found at {RESPONSES_JSON_PATH}. Please submit a question first."
        }), 500

    if evaluator_client is None:
        return jsonify({
            "status": "failure",
            "message": f"Evaluation LLM ({evaluator_model}) is not initialized."
        }), 500

    try:
        # Load the responses generated by the last /ask request
        with open(RESPONSES_JSON_PATH, "r", encoding="utf-8") as f:
            responses_dict = json.load(f)

        score_dict = {}
        # Use the responses for evaluation
        for i, (model_name, r) in enumerate(responses_dict.items()):
            prompt = (
                "You are a large language model specialized in evaluating the accuracy of the response of another large"
                f"language model to this question: '{user_question}'. Evaluate the following response from 1-100, "
                f"mentioning ONLY THE SCORE,"
                f"taking into consideration the general accuracy, but also the "
                f"fulfillment and compliance of the following instructions: '{instructions_response_llm}': {r}."
                f"DO NOT mention any explanation.")

            # get score from evaluator LLM
            response = receive_response_from_server_udp(UDP_SERVER_IP, UDP_SERVER_PORT, ["", prompt, evaluator_model])

            score_dict[model_name] = response

        # Save the scores to scores.json
        if save_json_format(score_dict, SCORES_JSON_PATH):
            return jsonify({
                "status": "success",
                "message": "Evaluation completed and scores saved.",
                "file_path": SCORES_JSON_PATH
            })
        else:
            return jsonify({
                "status": "failure",
                "message": "Evaluation completed but failed to save scores file."
            }), 500

    except Exception as exc:
        return jsonify({
            "status": "failure",
            "message": f"An unexpected error occurred during evaluation: {exc}"
        }), 500


if __name__ == '__main__':
    # Run the Flask app on .env host and port
    print("Starting Flask server...")
    app.run(host=FLASK_HOST, port=FLASK_PORT, debug=False, use_reloader=False)

